{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d79fc373-0ba0-4eb8-a8d7-49cab094cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda3858-fae0-46fa-9f5a-06a2233ed0b1",
   "metadata": {},
   "source": [
    "### Once we import pyspark, we need to use a `SparkContext`.  Every spark program needs a SparkContext object\n",
    "### In order to use DataFrames, we also need to import `SparkSession` from `pyspark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa5212a-5806-4d85-927b-2f9ef57ceaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5a45f-237f-48cc-a9a3-3e26a642fcd2",
   "metadata": {},
   "source": [
    "## We then create a Spark Session variable (rather than Spark Context) in order to use DataFrame. \n",
    "- Note: We temporarily use \"local\" as the parameter for master in this notebook so that we can test it in ICDS Roar Collab.  However, we need to remove \"local\" as usual to submit it to ICDS in cluster model (here make sure you remove \".master(\"local\")\" completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8538244a-2f05-4e71-83ed-c2b344dba1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"Modeling Logistic Regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76043d5c-d4d7-4bf3-b84d-063ac9165e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e00252bb-5ae3-4853-9f10-470cb1ac1894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone repository\n",
    "# !git clone https://brianellis1997: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@github.com/brianellis1997/Sarcasm_PySpark.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126e87e-fbf7-4a28-b80f-6d6ceb1ef38b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3465d59c-833e-47ee-a25b-ec272653f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), False),\n",
    "    StructField(\"label\", IntegerType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"ups\", IntegerType(), True),\n",
    "    StructField(\"downs\", IntegerType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"created_utc\", TimestampType(), True),\n",
    "    StructField(\"parent_comment\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5926500-16b9-4683-b3a1-c4424a376122",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ss.read.csv(\"/storage/home/bje5256/work/Project/Train_Balanced.csv\", header=True, schema=schema)\n",
    "# In the cluster mode, we need to change to  `header=False` because it does not have header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e633c6e-b1a2-4227-87aa-7ee32082fa3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0557c157-b084-4014-a48e-83d0fb657162",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Our goals for data preprocessing are as follows:\n",
    "<br>**Feature Engineering:**<br><ul>\n",
    "    <li>Cyclic date time variables like `month`, `day_of_week`, `hour`</li>\n",
    "    <li>Counting text information like `word_count`, `punctuation_count`</li>\n",
    "    <li>Quantify sentiment of sarcastic vs non-sarcastic text.</li>\n",
    "</ul>\n",
    "<br>**Transformations:**<br>\n",
    "This will generally consist of transforming our categorical and text covariates into numeric features our model will be able to understand.<ul>\n",
    "    <li>One-hot-encoding `subreddit`</li>\n",
    "    <li>Possibly generating tf-idf vectors of `comment`, `parent_comment`, and `subreddit`</li>\n",
    "</ul>\n",
    "<br>**Scaling and Splitting:**<br><ul>\n",
    "    <li>Standardize our variables</li>\n",
    "    <li>Split our train dataset into train and validation 80/20</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe75f93-8755-4fc3-b9cf-e8a99e98e87f",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Now that we have an idea of which variables are more important than the others, we can remove the unnecessary variables and add our feature engineered variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd9e878-a49a-4697-91d1-fc42f69ce586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing libraries\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f6f49b-049b-47c6-a7a0-50d4d934cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add date-time variables\n",
    "df2 = train.withColumn('month', F.month('created_utc')) \\\n",
    "           .withColumn('day_of_week', F.dayofweek('created_utc')) \\\n",
    "           .withColumn('hour', F.hour('created_utc'))\n",
    "\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26ec87e9-e379-40f7-a7a1-0aceac70b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of nulls in each row by checking each column\n",
    "null_check = df2.select([F.when(F.col(c).isNull(), 1).otherwise(0).alias(c) for c in df2.columns])\n",
    "\n",
    "# Sum up the values across all columns for each row, resulting in a new DataFrame where each row has a sum of nulls\n",
    "null_sums = null_check.withColumn('null_sum', sum(F.col(c) for c in null_check.columns))\n",
    "\n",
    "# Filter to get only the rows with at least one null value and count them\n",
    "num_rows_with_nulls = null_sums.filter(F.col('null_sum') > 0).count()\n",
    "\n",
    "# print(f\"Number of rows with at least one null value: {num_rows_with_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5474fa7e-c127-4785-8310-fe054136e7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows_with_nulls/df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f84a39d-6269-4a44-80d1-c01187c1a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9b5b6-18b9-4e2f-9d3d-75c24ff2b17c",
   "metadata": {},
   "source": [
    "Since the amount of rows with missing values is less than 1%, let's filter out these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6effe765-6db6-466e-88c5-eb8f0152c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.dropna()\n",
    "# df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376cf3ef-97a0-4b05-b7a2-6a7d25a550bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import string\n",
    "\n",
    "# text counting\n",
    "count_punctuation_udf = udf(lambda comment: sum(1 for char in comment if char in string.punctuation) if comment is not None else 0, IntegerType())\n",
    "count_capital_letters_udf = udf(lambda comment: sum(1 for char in comment if char.isupper()) if comment is not None else 0, IntegerType())\n",
    "\n",
    "# Add columns for counting punctuation marks and capital letters\n",
    "# comment\n",
    "df3 = df3.withColumn('word_count', udf(lambda x: len(x.split()) if x is not None else 0, IntegerType())(col('comment')))\n",
    "df3 = df3.withColumn('total_punctuation', count_punctuation_udf(col('comment')))\n",
    "\n",
    "# df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5186c50f-fce9-46a0-a7fa-9e20ed884e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of nulls in each row by checking each column\n",
    "null_check = df3.select([F.when(F.col(c).isNull(), 1).otherwise(0).alias(c) for c in df3.columns])\n",
    "\n",
    "# Sum up the values across all columns for each row, resulting in a new DataFrame where each row has a sum of nulls\n",
    "null_sums = null_check.withColumn('null_sum', sum(F.col(c) for c in null_check.columns))\n",
    "\n",
    "# Filter to get only the rows with at least one null value and count them\n",
    "num_rows_with_nulls = null_sums.filter(F.col('null_sum') > 0).count()\n",
    "\n",
    "# print(f\"Number of rows with at least one null value: {num_rows_with_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0faec6cf-de16-4b2d-a153-aece840fd4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "# df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1dff6b8-e495-4397-866c-346ec70b4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df3.select('label', 'comment', 'parent_comment', 'subreddit', 'score', 'month', 'day_of_week', 'hour', 'word_count', 'total_punctuation')\n",
    "# clean_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3298a-1fc5-4224-ad00-15a07138e1b9",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "Now that we have all the necessary features in our `clean_df`, we can start processing the data and performing transformations such as one-hot-encoding and maybe creating tf-idf vectors to input into our traditional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aafee199-81fb-4867-8935-11449e485189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/storage/home/bje5256/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/storage/home/bje5256/.local/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/swst/apps/anaconda3/2021.05_gcc-8.5.0/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9a15c0a1281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Subreddit value counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subreddit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[0;32m-> 1234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/swst/apps/anaconda3/2021.05_gcc-8.5.0/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Subreddit value counts\n",
    "clean_df.select('subreddit').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a017ff-7c97-4e5b-bab9-764d54a8e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'subreddit' and count the entries for each\n",
    "subreddit_counts = clean_df.groupBy('subreddit').count()\n",
    "\n",
    "# Step 2: Filter for subreddits with less than 5 comments\n",
    "subreddits_less_than_5 = subreddit_counts.filter(col('count') < 2)\n",
    "\n",
    "# Step 3: Count how many subreddits have less than 5 comments\n",
    "number_of_subreddits_less_than_5 = subreddits_less_than_5.count()\n",
    "\n",
    "# print(f\"Number of subreddits with less than 5 comments: {number_of_subreddits_less_than_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea178dff-a80e-4de1-a7e0-2a12b2717b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assuming your DataFrame is named clean_df and you have a SparkSession initialized\n",
    "\n",
    "# Step 1: Count comments per subreddit\n",
    "subreddit_counts = clean_df.groupBy('subreddit').count()\n",
    "\n",
    "# Step 2: Join this count back to the original dataframe to mark subreddits with less than 2 comments\n",
    "clean_df = clean_df.join(subreddit_counts, on=\"subreddit\", how=\"left\")\n",
    "\n",
    "# Replace subreddits with less than 2 comments with \"other\"\n",
    "clean_df = clean_df.withColumn(\"subreddit_modified\",\n",
    "                               when(col(\"count\") < 2, \"other\")\n",
    "                               .otherwise(col(\"subreddit\")))\n",
    "\n",
    "# Drop the original 'subreddit' and 'count' columns as they are no longer needed\n",
    "clean_df = clean_df.drop('subreddit', 'count')\n",
    "\n",
    "# Rename 'subreddit_modified' back to 'subreddit' for clarity\n",
    "clean_df = clean_df.withColumnRenamed(\"subreddit_modified\", \"subreddit\")\n",
    "\n",
    "# Step 3: One-hot-encode the modified subreddit column\n",
    "# First, convert categories into indices\n",
    "stringIndexer = StringIndexer(inputCol=\"subreddit\", outputCol=\"subredditIndex\")\n",
    "\n",
    "# Then apply OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"subredditIndex\"], outputCols=[\"subredditVec\"])\n",
    "\n",
    "# Use a Pipeline to apply the steps\n",
    "pipeline = Pipeline(stages=[stringIndexer, encoder])\n",
    "\n",
    "# Fit and transform the data\n",
    "model = pipeline.fit(clean_df)\n",
    "encoded_df = model.transform(clean_df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "# encoded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e161f14-b5c6-421e-861c-3a3a8be97b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df.select('subredditVec').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a2fd1-e1ff-4329-89c0-c5b98b207668",
   "metadata": {},
   "source": [
    "Vector Size (2718): This number represents the total number of unique subreddits (after processing, including the \"other\" category for subreddits with less than 2 comments) that have been identified across all comments in your dataset. It is the dimensionality of the one-hot encoded vector, meaning there are 2718 possible categories (subreddits) that each comment could belong to.\n",
    "\n",
    "Index ([906], [3], [84], etc.): This number represents the index within the vector that corresponds to the specific subreddit a comment is associated with. The index starts at 0, so an index of 906 refers to the 907th subreddit in the sorted list of unique subreddits. Each comment's subreddit is represented by one of these indices, indicating which subreddit the comment belongs to.\n",
    "\n",
    "Value ([1.0]): This indicates the value at the specified index. In the case of one-hot encoding, this will always be 1.0 for the index corresponding to the comment's subreddit, meaning the presence of that subreddit. All other positions in the vector will be 0 (not shown in the sparse vector representation), indicating the absence of those subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69b5aa-54d1-4a45-8250-77bc29d71ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can drop the subreddit feature\n",
    "transformed_df = encoded_df.drop('subreddit')\n",
    "# transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b39ce4-3f15-43a6-83d9-57ef3ee8af5b",
   "metadata": {},
   "source": [
    "In the context of the HashingTF transformer in PySpark, numFeatures specifies the number of features (or the size of the output feature vector) that you want to create for each document (in your case, each comment or parent_comment). This parameter is crucial for the \"feature hashing\" technique used by HashingTF.\n",
    "\n",
    "Feature hashing, also known as the hashing trick, is a method to map potentially infinite-dimensional features (e.g., words in text data) to a finite-dimensional vector space using a hash function. The hash function converts words to indices in the feature vector, where each index corresponds to a \"feature\" or \"bucket\". The value at each index in the vector represents the frequency (term frequency, TF) of the words that hash to that index.\n",
    "\n",
    "Pros: The primary advantage of feature hashing is its efficiency and scalability, as it allows for a fixed-size vector representation without needing to maintain a vocabulary in memory, which is particularly beneficial for large datasets.\n",
    "\n",
    "Cons: A limitation of this approach is the possibility of hash collisions, where different words are mapped to the same index, especially if numFeatures is too small relative to the diversity of the corpus. While some collisions are generally acceptable and do not significantly impact model performance in practice, setting numFeatures too low can lead to a loss of information and potentially degrade model performance.\n",
    "\n",
    "Using \n",
    "2^16\n",
    "  (or 65,536) as the number of features for a dataset with 800,000 rows can be a reasonable choice, especially when dealing with text data that can have a very large and sparse feature space. Here are a few considerations to keep in mind:\n",
    "\n",
    "Dimensionality vs. Dataset Size\n",
    "Sufficient Dimensionality: For text data, which often involves a large vocabulary, having a sufficiently high dimensionality for the feature space is crucial to reduce the risk of hash collisions (where different words are mapped to the same feature index). A value of \n",
    "2\n",
    "16\n",
    "2 \n",
    "16\n",
    "  offers a wide space that can accommodate a large vocabulary while keeping the collisions relatively low.\n",
    "Dataset Size: With 800,000 rows, your dataset is substantial. A larger numFeatures helps ensure that the nuanced differences in text across many samples can be captured without too much information loss due to collisions.\n",
    "Computational Considerations\n",
    "Memory and Speed: Larger numFeatures values will increase the memory usage and potentially the computation time for training models. However, Spark is designed to handle large-scale data processing, and feature vectors of size \n",
    "2\n",
    "16\n",
    "2 \n",
    "16\n",
    "  are generally manageable on modern hardware, especially when using Spark's distributed computing capabilities.\n",
    "Model Performance: The choice of numFeatures can affect model performance. Too small a space might lead to too many collisions, losing important information and possibly degrading model performance. Conversely, an excessively large space might increase computational overhead without proportional gains in model accuracy. \n",
    "2\n",
    "16\n",
    "2 \n",
    "16\n",
    "  is a good starting point, but it's always a good idea to experiment with different values if resources permit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d075f-5882-4249-adb5-7cc32eabc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create tf-idf vectors for our text comments\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Tokenize comment and parent_comment\n",
    "tokenizer_comment = Tokenizer(inputCol=\"comment\", outputCol=\"comment_tokens\")\n",
    "tokenizer_parent_comment = Tokenizer(inputCol=\"parent_comment\", outputCol=\"parent_comment_tokens\")\n",
    "\n",
    "# Apply HashingTF\n",
    "hashingTF_comment = HashingTF(inputCol=\"comment_tokens\", outputCol=\"rawFeatures_comment\", numFeatures=2**13)\n",
    "hashingTF_parent_comment = HashingTF(inputCol=\"parent_comment_tokens\", outputCol=\"rawFeatures_parent_comment\", numFeatures=2**13)\n",
    "\n",
    "# Compute IDF for each feature vector\n",
    "idf_comment = IDF(inputCol=\"rawFeatures_comment\", outputCol=\"features_comment\")\n",
    "idf_parent_comment = IDF(inputCol=\"rawFeatures_parent_comment\", outputCol=\"features_parent_comment\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer_comment, tokenizer_parent_comment, hashingTF_comment, hashingTF_parent_comment, idf_comment, idf_parent_comment])\n",
    "\n",
    "# Fit the pipeline to the dataset\n",
    "model = pipeline.fit(transformed_df)\n",
    "\n",
    "# Transform the dataset\n",
    "tfidf_df = model.transform(transformed_df)\n",
    "\n",
    "# Show the transformed features\n",
    "# tfidf_df.select(\"features_comment\", \"features_parent_comment\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887c830-3edd-4da5-84ae-eb608dcfe4e8",
   "metadata": {},
   "source": [
    "65536: This is the size of the vector, determined by the numFeatures parameter you set in the HashingTF step. It represents the total number of distinct hash values that can be produced by the hashing function. Each possible hash value corresponds to a \"bucket\" that can hold the count of one or more words, depending on whether hash collisions occur.\n",
    "\n",
    "[Indices]: These are the indices in the vector that have non-zero values. They represent the hash values of the words in the text, after the Tokenizer step has split the text into words and the HashingTF step has mapped these words to specific indices based on their hash values. Each index corresponds to a specific word (or multiple words in case of hash collisions).\n",
    "\n",
    "[Values]: These are the TF-IDF scores for the words at the corresponding indices. The TF-IDF score is a measure of how important a word is to a document in a collection of documents. It increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the collection, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "Tokenization: Words, Not Characters\n",
    "The Tokenizer step in PySpark ML splits the text into words, not characters. So, the TF-IDF vectors represent the importance (weight) of each word within the text, not each character. The default behavior of the Tokenizer is to split the text by white spaces, effectively treating each contiguous string of characters separated by spaces as a word.\n",
    "Interpretation of the Vectors\n",
    "Each entry in these vectors corresponds to a word's weighted importance in the text, with the weight computed based on the term's frequency across the document and its inverse document frequency across all documents. This means:\n",
    "\n",
    "Sparse Representation: Given that most documents contain only a small subset of the possible words, the TF-IDF vectors are sparse. This means that instead of storing a value for every possible word (which would be mostly zeros), it only stores values for words that actually appear in the text, significantly reducing memory usage.\n",
    "\n",
    "Hashing and Collisions: Since HashingTF uses a fixed-size vector to represent an potentially unlimited vocabulary, multiple words can end up being hashed to the same index, leading to what's known as a hash collision. While this can introduce some noise into the data, the high dimensionality (e.g., 65536) helps to minimize the impact of these collisions on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9143b2-46bf-4e9e-85f3-0a25f8541888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3722a4-6062-4fe5-952f-dbfe8b2ff9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a684d3e-2e04-4cb2-bab0-dfa848b7b2a7",
   "metadata": {},
   "source": [
    "## Feature Selection Insights\n",
    "\n",
    "### Potentially Useful Features\n",
    "\n",
    "- **`label`**: Essential for supervised learning as it's the target variable we will predict.\n",
    "- **`score`, `month`, `day_of_week`, `hour`**: These features could provide useful signals for our model, depending on the nature of our task. For instance, the time of posting might correlate with certain types of comments or their reception.\n",
    "- **`word_count`, `total_punctuation`**: These could serve as proxies for the length or complexity of a comment, which might be relevant for some analyses.\n",
    "- **`features_comment`, `features_parent_comment`**: The TF-IDF vectors are likely to be highly informative for text analysis or natural language processing tasks, as they represent the textual content in a numerical form that models can work with.\n",
    "\n",
    "### Features to Review or Exclude\n",
    "\n",
    "- **`subredditVec`**: This is the one-hot encoded representation of the subreddit. It's useful if we believe the subreddit context is important for our prediction task. However, we typically wouldn't need both `subredditVec` and `subredditIndex`.\n",
    "- **`subredditIndex`**: This is likely a numerical representation (index) of the subreddit used as an intermediate step for creating `subredditVec`. We would use either this or `subredditVec` for our model, not both, and `subredditVec` is usually the more useful form for machine learning models because it's one-hot encoded.\n",
    "\n",
    "### Intermediate Features (Usually Excluded from Modeling)\n",
    "\n",
    "- **`comment_tokens`, `parent_comment_tokens`**: These are intermediate representations used in the process of generating TF-IDF vectors. They're the tokenized lists of words from the comments and are not usually used directly in modeling once we have the TF-IDF vectors.\n",
    "- **`rawFeatures_comment`, `rawFeatures_parent_comment`**: These represent the hashed feature vectors (before applying IDF) and are intermediate steps towards generating the `features_comment` and `features_parent_comment` TF-IDF vectors. We would typically use the final TF-IDF vectors for modeling, not these intermediate hash vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ceba17-3310-46c1-a93b-c8eb424c7d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can drop the comment and parent comment since they are represented as tf-idf vectors\n",
    "final_df = tfidf_df.select('features_comment', 'features_parent_comment', 'subredditVec', 'score', 'month', 'day_of_week', 'hour', 'word_count', 'total_punctuation', 'label')\n",
    "# final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd118b7-cdc0-46ef-8b15-334b9066e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053599ce-8de3-4071-b1df-31b8e3ca1582",
   "metadata": {},
   "source": [
    "Now, we can move onto scaling and splitting our data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fedd84-f274-4e1e-a17e-9e073887989a",
   "metadata": {},
   "source": [
    "# Scaling and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29b971-a05d-4278-b6d9-bd23b21b4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of numerical columns to scale\n",
    "numericCols = ['score', 'month', 'day_of_week', 'hour', 'word_count', 'total_punctuation']\n",
    "\n",
    "# Assemble numerical features into a vector\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"numeric_features\")\n",
    "final_df = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a23d3-2637-4722-8ff4-73d277859abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(final_df)\n",
    "final_df = scalerModel.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff526e-456d-415a-b261-4d3eeef26a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59a7f0-6595-498c-8da9-5b1a68a9392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df = final_df.select('features_comment', \n",
    "                            'features_parent_comment', \n",
    "                            'subredditVec', \n",
    "                            'scaled_numeric_features', \n",
    "                            'label')\n",
    "# scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be10d95-8b97-4a93-97a7-d4bd5ecb212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = scaled_df.randomSplit([0.8, 0.2], seed=22)\n",
    "\n",
    "train_rows = train_df.count()\n",
    "train_cols = len(train_df.columns)\n",
    "\n",
    "val_rows = val_df.count()\n",
    "val_cols = len(val_df.columns)\n",
    "\n",
    "# print(f\"Shape of train_df: ({train_rows}, {train_cols})\")\n",
    "# print(f\"Shape of val_df: ({val_rows}, {val_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad721fd-87f3-4cfa-83b7-4ffd897d4e5a",
   "metadata": {},
   "source": [
    "### Train and Evaluate using only text covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7073612b-8d7e-4926-8b4f-60e47a277fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"features_comment\", \"features_parent_comment\"], outputCol=\"features\")\n",
    "\n",
    "# Transform the dataset to include a new column 'features' that combines 'features_comment' and 'features_parent_comment'\n",
    "combined_df = assembler.transform(scaled_df)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = combined_df.randomSplit([0.8, 0.2], seed=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b89c566d-5267-4b0c-ba8b-0b8416c7f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Fit the model on the training data\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18aeb07c-7d2c-494e-8bef-d3a095b51f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.5778788067509497\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Predict on the validation data\n",
    "predictions = lrModel.transform(val_df)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0e72127-f2e1-4330-9c8e-871ac223a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.579889638258737\n",
      "Recall: 0.5851998515035268\n",
      "F1 Score: 0.5825326435082533\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Convert predictions and labels to float type\n",
    "predictions = predictions.withColumn(\"label\", predictions[\"label\"].cast(FloatType()))\n",
    "predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(FloatType()))\n",
    "\n",
    "# Prepare the RDD required for MulticlassMetrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Calculate precision, recall, and F1 Score\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0, beta=1.0)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707add3-86dc-4c16-9734-e5662b39f1fe",
   "metadata": {},
   "source": [
    "# Text + Non-Text Covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663187f8-09a2-417e-9a60-5850890ab7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['features_comment', \n",
    "                            'features_parent_comment', \n",
    "                            'subredditVec', \n",
    "                            'scaled_numeric_features'], outputCol=\"features\")\n",
    "\n",
    "# Transform the dataset to include a new column 'features' that combines 'features_comment' and 'features_parent_comment'\n",
    "combined_df = assembler.transform(scaled_df)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = combined_df.randomSplit([0.8, 0.2], seed=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acadb2c1-6014-45f0-a5ed-62540ea2d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Fit the model on the training data\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3725e-1b57-4b49-bec8-6368b0743e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Predict on the validation data\n",
    "predictions = lrModel.transform(val_df)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Convert predictions and labels to float type\n",
    "predictions = predictions.withColumn(\"label\", predictions[\"label\"].cast(FloatType()))\n",
    "predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(FloatType()))\n",
    "\n",
    "# Prepare the RDD required for MulticlassMetrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Calculate precision, recall, and F1 Score\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0, beta=1.0)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1Score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae23b1-24f5-4b45-a22d-2f4624d770df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Initialize the randomForest model\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Fit the model on the training data\n",
    "rfModel = rf.fit(train_df)\n",
    "\n",
    "# Predict on the validation data\n",
    "predictions = rfModel.transform(val_df)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Convert predictions and labels to float type\n",
    "predictions = predictions.withColumn(\"label\", predictions[\"label\"].cast(FloatType()))\n",
    "predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(FloatType()))\n",
    "\n",
    "# Prepare the RDD required for MulticlassMetrics\n",
    "predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Calculate precision, recall, and F1 Score\n",
    "precision = metrics.precision(1.0)\n",
    "recall = metrics.recall(1.0)\n",
    "f1Score = metrics.fMeasure(1.0, beta=1.0)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1Score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58c0319-1f75-49df-9ee7-833d8844e547",
   "metadata": {},
   "source": [
    "# Multi-Layer-Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5bdf7e-ade5-4dc8-a6d6-d7d8ebac6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the MLP classifier without specifying layers yet\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, seed=1234, featuresCol='features', labelCol='label')\n",
    "\n",
    "# Setup a ParamGridBuilder to construct a grid of parameters\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(mlp.blockSize, [64, 128, 256])\n",
    "             .addGrid(mlp.layers, [[len(input_columns), 256, 128, 2], [len(input_columns), 128, 64, 2], [len(input_columns), 30, 30, 2]])\n",
    "             .build())\n",
    "\n",
    "# Define a CrossValidator to search over the grid\n",
    "crossval = CrossValidator(estimator=mlp,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\"),\n",
    "                          numFolds=3)  # Use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "# Fetch best model\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = bestModel.transform(val_df)\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Show accuracy and parameters of the best model\n",
    "print(f\"Best Model Accuracy: {accuracy}\")\n",
    "print(f\"Best Model Parameters: {bestModel.extractParamMap()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f23)",
   "language": "python",
   "name": "ds410_sp24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
