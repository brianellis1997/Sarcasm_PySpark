{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7e96c0-81e3-460c-9222-e8d21dafc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "import pyspark\n",
    "\n",
    "\n",
    "# ### Once we import pyspark, we need to use a `SparkContext`.  Every spark program needs a SparkContext object\n",
    "# ### In order to use DataFrames, we also need to import `SparkSession` from `pyspark.sql`\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType, TimestampType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# ## We then create a Spark Session variable (rather than Spark Context) in order to use DataFrame. \n",
    "# - Note: We temporarily use \"local\" as the parameter for master in this notebook so that we can test it in ICDS Roar Collab.  However, we need to remove \"local\" as usual to submit it to ICDS in cluster model (here make sure you remove \".master(\"local\")\" completely\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "ss=SparkSession.builder.master(\"local\").appName(\"Modeling Regression\").getOrCreate()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# # Clone repository\n",
    "# !git clone https://brianellis1997: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@github.com/brianellis1997/Sarcasm_PySpark.git\n",
    "\n",
    "\n",
    "# ## Load Data\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", IntegerType(), False),\n",
    "    StructField(\"label\", IntegerType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"subreddit\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"ups\", IntegerType(), True),\n",
    "    StructField(\"downs\", IntegerType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"created_utc\", TimestampType(), True),\n",
    "    StructField(\"parent_comment\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92131865-3c28-412b-9f9c-de794d66e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Step 1: Load the test data\n",
    "train_df = ss.read.csv(\"/storage/home/bje5256/work/Project/Train_Balanced.csv\", header=True, schema=schema)\n",
    "\n",
    "train_df = train_df.sample(withReplacement=False, fraction=0.05, seed=42)\n",
    "\n",
    "train = train_df.dropna()\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, IDF, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import string\n",
    "\n",
    "# Assume all initializations and schema definitions are done as per your setup\n",
    "\n",
    "# Feature engineering functions\n",
    "def get_month(dt):\n",
    "    return dt.month\n",
    "get_month_udf = udf(get_month, IntegerType())\n",
    "\n",
    "def get_day_of_week(dt):\n",
    "    return dt.dayofweek\n",
    "get_day_of_week_udf = udf(get_day_of_week, IntegerType())\n",
    "\n",
    "def get_hour(dt):\n",
    "    return dt.hour\n",
    "get_hour_udf = udf(get_hour, IntegerType())\n",
    "\n",
    "def count_punctuations(text):\n",
    "    return sum([1 for char in text if char in string.punctuation])\n",
    "count_punctuations_udf = udf(count_punctuations, IntegerType())\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer_comment = RegexTokenizer(inputCol=\"comment\", outputCol=\"comment_tokens\", pattern=\"\\\\W\")\n",
    "tokenizer_parent_comment = RegexTokenizer(inputCol=\"parent_comment\", outputCol=\"parent_comment_tokens\", pattern=\"\\\\W\")\n",
    "\n",
    "hashingTF_comment = HashingTF(inputCol=\"comment_tokens\", outputCol=\"rawFeatures_comment\", numFeatures=2**13)\n",
    "hashingTF_parent_comment = HashingTF(inputCol=\"parent_comment_tokens\", outputCol=\"rawFeatures_parent_comment\", numFeatures=2**13)\n",
    "\n",
    "idf_comment = IDF(inputCol=\"rawFeatures_comment\", outputCol=\"features_comment\")\n",
    "idf_parent_comment = IDF(inputCol=\"rawFeatures_parent_comment\", outputCol=\"features_parent_comment\")\n",
    "\n",
    "# For subreddit, use StringIndexer + OneHotEncoder with handleInvalid parameter set to 'keep'\n",
    "stringIndexer_subreddit = StringIndexer(inputCol=\"subreddit\", outputCol=\"subredditIndex\", handleInvalid=\"keep\")\n",
    "encoder_subreddit = OneHotEncoder(inputCols=[\"subredditIndex\"], outputCols=[\"subredditVec\"])\n",
    "\n",
    "\n",
    "# Combine all features into a single vector\n",
    "assembler_features = VectorAssembler(inputCols=[\"features_comment\", \"features_parent_comment\", \"subredditVec\"], outputCol=\"features\")\n",
    "\n",
    "# Define the full pipeline\n",
    "preprocessingPipeline = Pipeline(stages=[\n",
    "    tokenizer_comment, tokenizer_parent_comment,\n",
    "    hashingTF_comment, hashingTF_parent_comment,\n",
    "    idf_comment, idf_parent_comment,\n",
    "    stringIndexer_subreddit, encoder_subreddit,\n",
    "    assembler_features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016aef63-bc8e-4d69-811d-d5be29fc9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "preprocessingModel = preprocessingPipeline.fit(train)\n",
    "\n",
    "# Transform the training data\n",
    "preprocessedTrain = preprocessingModel.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "292cd749-ba64-4b16-b7ec-8224708008a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Step 1: Load the test data\n",
    "test_df = ss.read.csv(\"/storage/home/bje5256/work/Project/Test_Balanced.csv\", header=True, schema=schema)\n",
    "\n",
    "test_df = test_df.sample(withReplacement=False, fraction=0.05, seed=42)\n",
    "\n",
    "test = test_df.dropna()\n",
    "\n",
    "# Transform the test data using the fitted preprocessing model\n",
    "preprocessedTest = preprocessingModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f69559-dcb8-452b-b8d9-802f7107fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize and fit the logistic regression model on the preprocessed training data\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', regParam=0.1)\n",
    "lrModel = lr.fit(preprocessedTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74005093-bc84-4453-9f4c-b5d02958f1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.6135814889336016\n"
     ]
    }
   ],
   "source": [
    "# Predict on the preprocessed test data\n",
    "test_predictions = lrModel.transform(preprocessedTest)\n",
    "\n",
    "# Evaluate the model's accuracy on the test data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "test_accuracy = evaluator.evaluate(test_predictions)\n",
    "print(\"Accuracy on test data:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359b668-e6b0-4961-86f5-26e664f48c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f23)",
   "language": "python",
   "name": "ds410_sp24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
